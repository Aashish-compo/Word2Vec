{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1264f21a",
   "metadata": {},
   "source": [
    "## Word2vec\n",
    "**Word2vec** is a popular technique used in natural language processing to represent words as numerical vectors. It is a neural network-based method that learns word embeddings from large amounts of text data. Word2vec has become an important tool in the field of natural language processing and has helped to advance the development of more advanced language technologies.\n",
    "\n",
    "Word2vec works by considering the context in which words appear in a text corpus. The neural network is trained to predict the probability of a word appearing in a given context, given the surrounding words. This allows the network to learn word embeddings that capture the relationships between words in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc9258a",
   "metadata": {},
   "source": [
    "### We'll be using Game of thrones Books dataset for conversion of words to vector.\n",
    "\n",
    "### We'll be using gensim and nltk library for the manipulation of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8217c402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\ashis\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: regex in c:\\users\\ashis\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashis\\appdata\\roaming\\python\\python38\\site-packages (from click->nltk) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Ashis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#Importing important libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import os\n",
    "import nltk\n",
    "!pip install nltk\n",
    "from nltk import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90280ec7",
   "metadata": {},
   "source": [
    "we have downloaded the dataset of game of throne books from kaggle and we perform following operations:\n",
    "\n",
    "- Fetch each file\n",
    "- read each file\n",
    "- Perform preprocessing and put each preproprocessed sentence in a list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "167606e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "story = []\n",
    "for filename in os.listdir('data'):\n",
    "    \n",
    "    f = open(os.path.join('data',filename))\n",
    "    \n",
    "    #read file\n",
    "    corpus = f.read()\n",
    "    \n",
    "    #put all sentence in list\n",
    "    raw_sent = sent_tokenize(corpus)\n",
    "    \n",
    "    #Basic preprocess each sentence \n",
    "    for sent in raw_sent:\n",
    "        story.append(simple_preprocess(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efa0e346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['game', 'of', 'thrones', 'book', 'one', 'of', 'song', 'of', 'ice', 'and', 'fire', 'by', 'george', 'martin', 'prologue', 'we', 'should', 'start', 'back', 'gared', 'urged', 'as', 'the', 'woods', 'began', 'to', 'grow', 'dark', 'around', 'them'], ['the', 'wildlings', 'are', 'dead'], ['do', 'the', 'dead', 'frighten', 'you'], ['ser', 'waymar', 'royce', 'asked', 'with', 'just', 'the', 'hint', 'of', 'smile'], ['gared', 'did', 'not', 'rise', 'to', 'the', 'bait'], ['he', 'was', 'an', 'old', 'man', 'past', 'fifty', 'and', 'he', 'had', 'seen', 'the', 'lordlings', 'come', 'and', 'go']]\n"
     ]
    }
   ],
   "source": [
    "# Let's print first six sentence from our story list\n",
    "print([story[i] for i in range(6)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a04af",
   "metadata": {},
   "source": [
    "### Under class Word2Vec which is inside the gensim.models is used to make model\n",
    "We are going to use three parameters \n",
    "\n",
    "- **window**: Window is used to define how many words can be used to make predections of center words and on both side we use 10 10 words for the predictions if window = 10\n",
    "\n",
    "- **min_count**: we use only those sentences which have the minimum word 2 if min_count = 2\n",
    "\n",
    "- **vector_size**: tells the model how many size of vector we want to make for each words. Basically hidden layers play important role for this.(see in docs below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4425a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "        window=10,\n",
    "        min_count=2,\n",
    "        vector_size = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c12aa44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build our vocabulary from the story\n",
    "model.build_vocab(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "578f94bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6570597, 8628190)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's train our model\n",
    "model.train(story, total_examples=model.corpus_count, epochs = model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ef8b81",
   "metadata": {},
   "source": [
    "### We have trained our model using model.train now we can explore our trained dataset\n",
    "\n",
    "We can use model.wv.func() for the manipulating models and words.\n",
    "eg we can use **model.wv.most_similar(word)** to get most similar word fora certain words and alsovisualise the vector of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a6432f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('started', 0.721495509147644), ('went', 0.6500563025474548), ('gets', 0.6296276450157166), ('tried', 0.6125317215919495), ('stole', 0.5981466770172119), ('squirting', 0.595348060131073), ('goes', 0.5852327346801758), ('came', 0.5687958598136902), ('moved', 0.5684332251548767), ('shouted', 0.5670421719551086)]\n",
      "[-0.65933394  1.5233262   1.669839    0.5993785   1.1982256   2.2859757\n",
      "  0.24119651 -0.4587817   0.8062619   0.0499928   2.5661209   0.39831415\n",
      "  0.50965524  0.10206635  0.21645388 -0.2893774   0.3961005   0.6720759\n",
      " -1.6263245  -1.3243555   0.30309412  0.13111965 -1.3896111  -1.1457307\n",
      " -2.438746   -1.2344465   0.41160107 -0.3624295  -1.3560109   1.7957324\n",
      " -0.5155781   0.03727447  0.90332806  1.4986315   0.97128665 -0.1682729\n",
      "  0.1172601  -0.702237    0.05568082  0.58727115  0.9436814  -0.38819495\n",
      "  0.44841886  1.2588859   2.804377    0.12495188  0.7907396  -0.01418527\n",
      " -0.15780063  0.79143685  1.4343985  -0.7822081   0.4649995   1.5629027\n",
      " -0.41865376  0.52656144  0.81241685 -1.1490637   1.2011318  -0.31985876\n",
      " -1.7666656   0.7125544   0.43203735  0.92571306 -1.192574   -0.01001082\n",
      " -1.8643265  -2.1836386  -1.931583   -1.775393   -0.6290336  -0.5999783\n",
      "  0.883542    1.8647214   0.31111497  1.1454424  -0.17060794  0.4236639\n",
      " -1.8691199  -0.49792758  0.5227911  -1.7649258   1.104059    0.99909765\n",
      "  1.1269094   2.679147   -1.555464   -0.72220385  2.011793    0.5120281\n",
      " -0.92189     0.22136092  0.11924857 -0.5863116  -0.57466584 -0.73981607\n",
      " -0.5349423  -2.2460291   1.1482276  -0.42334774  0.3151897  -0.3734969\n",
      "  0.39876077 -0.83646315  0.40734065  1.6055119  -1.1651828   1.4243163\n",
      " -0.45387915  2.4042485   0.46998703 -0.20140363 -0.45082858  0.6526279\n",
      " -0.24779932 -0.8620054  -0.6250614  -2.5525904  -1.1350967  -0.9555718\n",
      "  1.3134921  -0.40502614 -0.74661815  1.6703874   0.17020337 -1.0763401\n",
      " -0.4621026   0.28643826  0.39977628 -0.50576097  0.3133982  -0.3642714\n",
      " -0.3880202  -0.75439054 -1.6180422   0.32656512  0.00680779  0.3264916\n",
      "  0.09810373 -1.2004721  -0.6590732  -0.5016881  -0.28642666 -0.61222494\n",
      "  0.03302844  0.16531968  0.86385274  0.78565055  0.5527692   1.171361\n",
      " -1.1807498   0.773086   -1.2651114  -1.9384712   0.73395413 -0.5396272\n",
      " -2.2200186   0.44128346  0.64534616  1.0721036  -0.4244247  -0.92832106\n",
      "  0.26203436  1.04958     0.9293797   1.0592886   0.3190916   1.0781406\n",
      "  1.4670392  -0.22323541 -0.66365945  2.2395542  -1.3943332   0.23553509\n",
      "  0.44504544  0.41515076 -0.06541266  0.38343576  0.9938889  -0.1062101\n",
      "  1.6401247  -0.2836488   1.8151342  -0.5356708   0.70944065  1.1865531\n",
      "  0.48155677 -0.62532914  0.2677535  -0.99336886 -0.48799953  1.0655271\n",
      "  0.12062354 -2.0469508   0.22953258 -1.6561362   0.17395292  0.42338806\n",
      "  0.3090515  -1.56912   ]\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(\"got\"))\n",
    "\n",
    "print(model.wv[\"man\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201ccf1e",
   "metadata": {},
   "source": [
    "Word2Vec is considered a good word embedding technique for several reasons:\n",
    "\n",
    "- It captures semantic and syntactic relationships between words: Word2Vec uses a neural network to learn representations of words that are close to each other in meaning or context. This allows it to capture semantic relationships between words, such as \"king\" and \"queen\", which are likely to appear in similar contexts. It can also capture syntactic relationships, such as the relationship between \"walk\" and \"walked\".\n",
    "\n",
    "\n",
    "- It can handle large vocabularies: Word2Vec can efficiently handle vocabularies with hundreds of thousands of words, which is important for many natural language processing tasks.\n",
    "\n",
    "\n",
    "\n",
    "- It is computationally efficient: Word2Vec uses a technique called negative sampling to train the neural network, which makes it more computationally efficient than other techniques like neural language models.\n",
    "\n",
    "\n",
    "\n",
    "- It can be pre-trained on large corpora: Word2Vec can be pre-trained on large corpora of text, such as Wikipedia or the entire internet. This pre-training can help improve performance on downstream tasks that have limited training data.\n",
    "\n",
    "Overall, Word2Vec is a powerful word embedding technique that has been widely used and proven effective for a variety of natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964d9add",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9b8fe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
